{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotionrecognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqwwi88qTuOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "#Loading the dataset\n",
        "dataset = pd.read_csv(\"../input/emotion.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94I_Y4vYTwAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.emotions.value_counts().plot.bar()\n",
        "dataset.head(10)\n",
        "input_sentences = [text.split(\" \") for text in dataset[\"text\"].values.tolist()]\n",
        "labels = dataset[\"emotions\"].values.tolist()\n",
        "\n",
        "word2id = dict()\n",
        "label2id = dict()\n",
        "\n",
        "max_words = 0 # maximum number of words in a sentence\n",
        "\n",
        "# Construction of word2id dict\n",
        "for sentence in input_sentences:\n",
        "    for word in sentence:\n",
        "        # Add words to word2id dict if not exist\n",
        "        if word not in word2id:\n",
        "            word2id[word] = len(word2id)\n",
        "    # If length of the sentence is greater than max_words, update max_words\n",
        "    if len(sentence) > max_words:\n",
        "        max_words = len(sentence)\n",
        "    \n",
        "# Construction of label2id and id2label dicts\n",
        "label2id = {l: i for i, l in enumerate(set(labels))}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "id2label\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MI_7ZHGUfZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "# Encode input words and labels\n",
        "X = [[word2id[word] for word in sentence] for sentence in input_sentences]\n",
        "Y = [label2id[label] for label in labels]\n",
        "\n",
        "# Apply Padding to X\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = pad_sequences(X, max_words)\n",
        "\n",
        "# Convert Y to numpy array\n",
        "Y = keras.utils.to_categorical(Y, num_classes=len(label2id), dtype='float32')\n",
        "\n",
        "# Print shapes\n",
        "print(\"Shape of X: {}\".format(X.shape))\n",
        "print(\"Shape of Y: {}\".format(Y.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfHjnHQ_UJ13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "# Select random samples to illustrate\n",
        "sample_text = random.choice(dataset[\"text\"].values.tolist())\n",
        "\n",
        "# Encode samples\n",
        "tokenized_sample = sample_text.split(\" \")\n",
        "encoded_samples = [[word2id[word] for word in tokenized_sample]]\n",
        "\n",
        "# Padding\n",
        "encoded_samples = keras.preprocessing.sequence.pad_sequences(encoded_samples, maxlen=max_words)\n",
        "\n",
        "# Make predictions\n",
        "label_probs, attentions = model_with_attentions.predict(encoded_samples)\n",
        "label_probs = {id2label[_id]: prob for (label, _id), prob in zip(label2id.items(),label_probs[0])}\n",
        "\n",
        "# Get word attentions using attenion vector\n",
        "token_attention_dic = {}\n",
        "max_score = 0.0\n",
        "min_score = 0.0\n",
        "for token, attention_score in zip(tokenized_sample, attentions[0][-len(tokenized_sample):]):\n",
        "    token_attention_dic[token] = math.sqrt(attention_score)\n",
        "\n",
        "\n",
        "# VISUALIZATION\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def rgb_to_hex(rgb):\n",
        "    return '#%02x%02x%02x' % rgb\n",
        "    \n",
        "def attention2color(attention_score):\n",
        "    r = 255 - int(attention_score * 255)\n",
        "    color = rgb_to_hex((255, r, r))\n",
        "    return str(color)\n",
        "\n",
        "\n",
        "# EMOTION SCORES\n",
        "emotions = [label for label, _ in label_probs.items()]\n",
        "scores = [score for _, score in label_probs.items()]\n",
        "plt.figure(figsize=(5,2))\n",
        "plt.bar(np.arange(len(emotions)), scores, align='center', alpha=0.5, color=['black', 'red', 'green', 'blue', 'cyan', \"purple\"])\n",
        "plt.xticks(np.arange(len(emotions)), emotions)\n",
        "plt.ylabel('Scores')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}